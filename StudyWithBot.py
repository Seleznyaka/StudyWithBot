import streamlit as st
from langchain_ollama import ChatOllama

# Инициализация модели
model = ChatOllama(model="gemma2:2b")

def generate_response(user_input):
    special_message = (
        "\n\n Пожалуйста, не пиши код на языках программирования. Используй только псевдокод с объяснениями. В случае необходимости, ты можешь написать максимум 1-2 строчки кода как пример."
        "Отвечай на русском языке и всегда обращайся ко мне как 'братанчик'."
        "Ты должен помогать мне в изучении программирования, активно обсуждая различные концепции и подходы."
        "Если я прошу решить задачу, предоставь псевдокод и задавай наводящие вопросы, чтобы лучше понять мою задачу."
        "Твоя основная цель — поддержать меня в учебном процессе, создавая периодические квизы и предлагая дополнительные объяснения в зависимости от запроса."
        "Каждый раз, когда ты предоставляешь информацию, напоминай мне, что ты всегда открыт для дополнительных вопросов и готов помочь."
    )

    question = user_input + special_message
    response = model.invoke(input=question)

    return response

# Основной код для Streamlit
st.title("Чат-бот для изучения программирования")
st.markdown("""
Это чат-бот, который поможет тебе в изучении программирования. 
Напиши что-нибудь, и бот ответит.
""")

# Создание текстового поля для ввода пользователя
user_input = st.text_area("Введите ваш текст:", height=200)

if user_input:
    bot_response = generate_response(user_input)
    
    # Отображение ответа от бота с поддержкой Markdown
    st.markdown(f"**Бот**: {bot_response.content}")

# Кнопка для выхода из чата (опционально)
if st.button("Выход"):
    st.text("Спасибо за общение! До свидания.")
    st.stop()
